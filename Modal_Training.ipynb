{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVCyxw3KbJPD"
   },
   "source": [
    "## Model Training \n",
    "    \n",
    "   - Now creating the model for training.\n",
    "   - Using the keras 2.2.4\n",
    "   - tensorflow 1.5\n",
    "   - opencv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DudVsVMFaCdj"
   },
   "source": [
    "**TRAINING THE DATA SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jSxdb5FGhoCR",
    "outputId": "ba51c9d9-532d-4af2-864e-cacbd9f6b751"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "R2bc23v97hr5",
    "outputId": "7e0df73f-1b7c-4ebb-ffd2-77339988a00e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-c8caf724-a174-4684-bfb1-02765ad32530\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-c8caf724-a174-4684-bfb1-02765ad32530\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving x.pickle to x.pickle\n",
      "User uploaded file \"x.pickle\" with length 103755766 bytes\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "EB8Vt0fY71Ts",
    "outputId": "d8c5f6c3-01c1-48d6-85b8-56fea9e80312"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-8a358357-e1db-4ff1-ac50-6b57f326e596\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-8a358357-e1db-4ff1-ac50-6b57f326e596\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving y.pickle to y.pickle\n",
      "User uploaded file \"y.pickle\" with length 57706 bytes\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEspSOiIaPZ-"
   },
   "source": [
    "**Loading the x and y pickle file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kml-J4EEhoCe"
   },
   "outputs": [],
   "source": [
    "x= pickle.load(open(\"x.pickle\",\"rb\"))\n",
    "y= pickle.load(open(\"y.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4f723C_aVyc"
   },
   "source": [
    "**Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3692
    },
    "colab_type": "code",
    "id": "potVZlbShoCp",
    "outputId": "bf6f0564-4288-4e7e-8083-9f4e0f828ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25938 samples, validate on 2883 samples\n",
      "Epoch 1/50\n",
      "25938/25938 [==============================] - 29s 1ms/step - loss: 1.7974 - acc: 0.2543 - val_loss: 1.7658 - val_acc: 0.2660\n",
      "Epoch 2/50\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 1.6653 - acc: 0.3381 - val_loss: 1.5829 - val_acc: 0.3815\n",
      "Epoch 3/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 1.5374 - acc: 0.4029 - val_loss: 1.5134 - val_acc: 0.4152\n",
      "Epoch 4/50\n",
      "25938/25938 [==============================] - 24s 913us/step - loss: 1.4553 - acc: 0.4407 - val_loss: 1.4332 - val_acc: 0.4475\n",
      "Epoch 5/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 1.3902 - acc: 0.4704 - val_loss: 1.4080 - val_acc: 0.4589\n",
      "Epoch 6/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 1.3313 - acc: 0.4952 - val_loss: 1.4051 - val_acc: 0.4468\n",
      "Epoch 7/50\n",
      "25938/25938 [==============================] - 24s 909us/step - loss: 1.2762 - acc: 0.5217 - val_loss: 1.3570 - val_acc: 0.4714\n",
      "Epoch 8/50\n",
      "25938/25938 [==============================] - 24s 914us/step - loss: 1.2235 - acc: 0.5402 - val_loss: 1.3697 - val_acc: 0.4818\n",
      "Epoch 9/50\n",
      "25938/25938 [==============================] - 24s 913us/step - loss: 1.1770 - acc: 0.5609 - val_loss: 1.3706 - val_acc: 0.4811\n",
      "Epoch 10/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 1.1302 - acc: 0.5785 - val_loss: 1.3778 - val_acc: 0.4932\n",
      "Epoch 11/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 1.0782 - acc: 0.5973 - val_loss: 1.3850 - val_acc: 0.4804\n",
      "Epoch 12/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 1.0350 - acc: 0.6160 - val_loss: 1.4111 - val_acc: 0.4839\n",
      "Epoch 13/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.9864 - acc: 0.6354 - val_loss: 1.4696 - val_acc: 0.4974\n",
      "Epoch 14/50\n",
      "20128/25938 [======================>.......] - ETA: 5s - loss: 0.9332 - acc: 0.6552WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25938 samples, validate on 2883 samples\n",
      "Epoch 1/50\n",
      "25938/25938 [==============================] - 29s 1ms/step - loss: 1.7974 - acc: 0.2543 - val_loss: 1.7658 - val_acc: 0.2660\n",
      "Epoch 2/50\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 1.6653 - acc: 0.3381 - val_loss: 1.5829 - val_acc: 0.3815\n",
      "Epoch 3/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 1.5374 - acc: 0.4029 - val_loss: 1.5134 - val_acc: 0.4152\n",
      "Epoch 4/50\n",
      "25938/25938 [==============================] - 24s 913us/step - loss: 1.4553 - acc: 0.4407 - val_loss: 1.4332 - val_acc: 0.4475\n",
      "Epoch 5/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 1.3902 - acc: 0.4704 - val_loss: 1.4080 - val_acc: 0.4589\n",
      "Epoch 6/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 1.3313 - acc: 0.4952 - val_loss: 1.4051 - val_acc: 0.4468\n",
      "Epoch 7/50\n",
      "25938/25938 [==============================] - 24s 909us/step - loss: 1.2762 - acc: 0.5217 - val_loss: 1.3570 - val_acc: 0.4714\n",
      "Epoch 8/50\n",
      "25938/25938 [==============================] - 24s 914us/step - loss: 1.2235 - acc: 0.5402 - val_loss: 1.3697 - val_acc: 0.4818\n",
      "Epoch 9/50\n",
      "25938/25938 [==============================] - 24s 913us/step - loss: 1.1770 - acc: 0.5609 - val_loss: 1.3706 - val_acc: 0.4811\n",
      "Epoch 10/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 1.1302 - acc: 0.5785 - val_loss: 1.3778 - val_acc: 0.4932\n",
      "Epoch 11/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 1.0782 - acc: 0.5973 - val_loss: 1.3850 - val_acc: 0.4804\n",
      "Epoch 12/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 1.0350 - acc: 0.6160 - val_loss: 1.4111 - val_acc: 0.4839\n",
      "Epoch 13/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.9864 - acc: 0.6354 - val_loss: 1.4696 - val_acc: 0.4974\n",
      "Epoch 14/50\n",
      "25938/25938 [==============================] - 24s 909us/step - loss: 0.9415 - acc: 0.6514 - val_loss: 1.4903 - val_acc: 0.4676\n",
      "25938/25938 [==============================] - 24s 909us/step - loss: 0.9415 - acc: 0.6514 - val_loss: 1.4903 - val_acc: 0.4676\n",
      "Epoch 15/50\n",
      "   32/25938 [..............................] - ETA: 25s - loss: 0.5687 - acc: 0.8438Epoch 15/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.8967 - acc: 0.6710 - val_loss: 1.4966 - val_acc: 0.4835\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.8967 - acc: 0.6710 - val_loss: 1.4966 - val_acc: 0.4835\n",
      "Epoch 16/50\n",
      "   32/25938 [..............................] - ETA: 23s - loss: 0.7553 - acc: 0.6875Epoch 16/50\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 0.8479 - acc: 0.6883 - val_loss: 1.5406 - val_acc: 0.4925\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 0.8479 - acc: 0.6883 - val_loss: 1.5406 - val_acc: 0.4925\n",
      "Epoch 17/50\n",
      "   32/25938 [..............................] - ETA: 25s - loss: 0.7638 - acc: 0.7188Epoch 17/50\n",
      "25938/25938 [==============================] - 24s 913us/step - loss: 0.8053 - acc: 0.7051 - val_loss: 1.6034 - val_acc: 0.4804\n",
      "25938/25938 [==============================] - 24s 913us/step - loss: 0.8053 - acc: 0.7051 - val_loss: 1.6034 - val_acc: 0.4804\n",
      "Epoch 18/50\n",
      "   32/25938 [..............................] - ETA: 25s - loss: 0.6492 - acc: 0.7188Epoch 18/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.7678 - acc: 0.7188 - val_loss: 1.6363 - val_acc: 0.4762\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.7678 - acc: 0.7188 - val_loss: 1.6363 - val_acc: 0.4762\n",
      "Epoch 19/50\n",
      "   32/25938 [..............................] - ETA: 23s - loss: 0.6962 - acc: 0.7188Epoch 19/50\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.7283 - acc: 0.7321 - val_loss: 1.7539 - val_acc: 0.4596\n",
      "25938/25938 [==============================] - 24s 911us/step - loss: 0.7283 - acc: 0.7321 - val_loss: 1.7539 - val_acc: 0.4596\n",
      "Epoch 20/50\n",
      "   32/25938 [..............................] - ETA: 23s - loss: 0.5937 - acc: 0.7812Epoch 20/50\n",
      "25938/25938 [==============================] - 24s 906us/step - loss: 0.6828 - acc: 0.7489 - val_loss: 1.8317 - val_acc: 0.4745\n",
      "25938/25938 [==============================] - 24s 906us/step - loss: 0.6828 - acc: 0.7489 - val_loss: 1.8317 - val_acc: 0.4745\n",
      "Epoch 21/50\n",
      "   32/25938 [..............................] - ETA: 22s - loss: 0.5208 - acc: 0.8438Epoch 21/50\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 0.6501 - acc: 0.7617 - val_loss: 1.8389 - val_acc: 0.4866\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 0.6501 - acc: 0.7617 - val_loss: 1.8389 - val_acc: 0.4866\n",
      "Epoch 22/50\n",
      "   32/25938 [..............................] - ETA: 24s - loss: 0.5578 - acc: 0.7500Epoch 22/50\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 0.6118 - acc: 0.7801 - val_loss: 1.9601 - val_acc: 0.4669\n",
      "25938/25938 [==============================] - 24s 912us/step - loss: 0.6118 - acc: 0.7801 - val_loss: 1.9601 - val_acc: 0.4669\n",
      "Epoch 23/50\n",
      "   32/25938 [..............................] - ETA: 24s - loss: 0.4331 - acc: 0.8125Epoch 23/50\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 0.5762 - acc: 0.7913 - val_loss: 2.0757 - val_acc: 0.4731\n",
      "25938/25938 [==============================] - 24s 910us/step - loss: 0.5762 - acc: 0.7913 - val_loss: 2.0757 - val_acc: 0.4731\n",
      "Epoch 24/50\n",
      "   32/25938 [..............................] - ETA: 24s - loss: 0.2997 - acc: 0.8750Epoch 24/50\n",
      "25938/25938 [==============================] - 25s 951us/step - loss: 0.5491 - acc: 0.8024 - val_loss: 2.1421 - val_acc: 0.4655\n",
      "25938/25938 [==============================] - 25s 951us/step - loss: 0.5491 - acc: 0.8024 - val_loss: 2.1421 - val_acc: 0.4655\n",
      "Epoch 25/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.4243 - acc: 0.8125Epoch 25/50\n",
      "25938/25938 [==============================] - 25s 967us/step - loss: 0.5189 - acc: 0.8109 - val_loss: 2.2400 - val_acc: 0.4693\n",
      "25938/25938 [==============================] - 25s 967us/step - loss: 0.5189 - acc: 0.8109 - val_loss: 2.2400 - val_acc: 0.4693\n",
      "Epoch 26/50\n",
      "   32/25938 [..............................] - ETA: 26s - loss: 0.6368 - acc: 0.7812Epoch 26/50\n",
      "25938/25938 [==============================] - 24s 918us/step - loss: 0.4988 - acc: 0.8171 - val_loss: 2.3746 - val_acc: 0.4631\n",
      "25938/25938 [==============================] - 24s 918us/step - loss: 0.4988 - acc: 0.8171 - val_loss: 2.3746 - val_acc: 0.4631\n",
      "Epoch 27/50\n",
      "   32/25938 [..............................] - ETA: 28s - loss: 0.3964 - acc: 0.9062Epoch 27/50\n",
      "25938/25938 [==============================] - 24s 919us/step - loss: 0.4602 - acc: 0.8322 - val_loss: 2.5285 - val_acc: 0.4547\n",
      "25938/25938 [==============================] - 24s 919us/step - loss: 0.4602 - acc: 0.8322 - val_loss: 2.5285 - val_acc: 0.4547\n",
      "Epoch 28/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.3764 - acc: 0.8438Epoch 28/50\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.4355 - acc: 0.8467 - val_loss: 2.5230 - val_acc: 0.4568\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.4355 - acc: 0.8467 - val_loss: 2.5230 - val_acc: 0.4568\n",
      "Epoch 29/50\n",
      "   32/25938 [..............................] - ETA: 26s - loss: 0.5761 - acc: 0.7812Epoch 29/50\n",
      "25938/25938 [==============================] - 24s 920us/step - loss: 0.4192 - acc: 0.8478 - val_loss: 2.5655 - val_acc: 0.4707\n",
      "25938/25938 [==============================] - 24s 920us/step - loss: 0.4192 - acc: 0.8478 - val_loss: 2.5655 - val_acc: 0.4707\n",
      "Epoch 30/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.5402 - acc: 0.8750Epoch 30/50\n",
      "25938/25938 [==============================] - 24s 920us/step - loss: 0.4067 - acc: 0.8513 - val_loss: 2.7620 - val_acc: 0.4762\n",
      "25938/25938 [==============================] - 24s 920us/step - loss: 0.4067 - acc: 0.8513 - val_loss: 2.7620 - val_acc: 0.4762\n",
      "Epoch 31/50\n",
      "   32/25938 [..............................] - ETA: 28s - loss: 0.4937 - acc: 0.8438Epoch 31/50\n",
      "25938/25938 [==============================] - 24s 923us/step - loss: 0.3727 - acc: 0.8644 - val_loss: 2.8761 - val_acc: 0.4579\n",
      "25938/25938 [==============================] - 24s 923us/step - loss: 0.3727 - acc: 0.8644 - val_loss: 2.8761 - val_acc: 0.4579\n",
      "Epoch 32/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.2707 - acc: 0.8750Epoch 32/50\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.3660 - acc: 0.8671 - val_loss: 2.9394 - val_acc: 0.4634\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.3660 - acc: 0.8671 - val_loss: 2.9394 - val_acc: 0.4634\n",
      "Epoch 33/50\n",
      "   32/25938 [..............................] - ETA: 29s - loss: 0.4175 - acc: 0.8125Epoch 33/50\n",
      "25938/25938 [==============================] - 24s 916us/step - loss: 0.3481 - acc: 0.8744 - val_loss: 2.9220 - val_acc: 0.4599\n",
      "25938/25938 [==============================] - 24s 916us/step - loss: 0.3481 - acc: 0.8744 - val_loss: 2.9220 - val_acc: 0.4599\n",
      "Epoch 34/50\n",
      "   32/25938 [..............................] - ETA: 24s - loss: 0.2209 - acc: 0.8750Epoch 34/50\n",
      "25938/25938 [==============================] - 24s 924us/step - loss: 0.3281 - acc: 0.8806 - val_loss: 3.0721 - val_acc: 0.4575\n",
      "25938/25938 [==============================] - 24s 924us/step - loss: 0.3281 - acc: 0.8806 - val_loss: 3.0721 - val_acc: 0.4575\n",
      "Epoch 35/50\n",
      "   32/25938 [..............................] - ETA: 26s - loss: 0.1654 - acc: 0.9375Epoch 35/50\n",
      "25938/25938 [==============================] - 24s 924us/step - loss: 0.3240 - acc: 0.8839 - val_loss: 3.0857 - val_acc: 0.4606\n",
      "25938/25938 [==============================] - 24s 924us/step - loss: 0.3240 - acc: 0.8839 - val_loss: 3.0857 - val_acc: 0.4606\n",
      "Epoch 36/50\n",
      "   32/25938 [..............................] - ETA: 28s - loss: 0.1275 - acc: 0.9688Epoch 36/50\n",
      "25938/25938 [==============================] - 24s 922us/step - loss: 0.3063 - acc: 0.8906 - val_loss: 3.2809 - val_acc: 0.4471\n",
      "25938/25938 [==============================] - 24s 922us/step - loss: 0.3063 - acc: 0.8906 - val_loss: 3.2809 - val_acc: 0.4471\n",
      "Epoch 37/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.2592 - acc: 0.9062Epoch 37/50\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.2875 - acc: 0.8952 - val_loss: 3.3024 - val_acc: 0.4648\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.2875 - acc: 0.8952 - val_loss: 3.3024 - val_acc: 0.4648\n",
      "Epoch 38/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.0915 - acc: 0.9688Epoch 38/50\n",
      "25938/25938 [==============================] - 24s 923us/step - loss: 0.2743 - acc: 0.9016 - val_loss: 3.4372 - val_acc: 0.4672\n",
      "25938/25938 [==============================] - 24s 923us/step - loss: 0.2743 - acc: 0.9016 - val_loss: 3.4372 - val_acc: 0.4672\n",
      "Epoch 39/50\n",
      "   32/25938 [..............................] - ETA: 28s - loss: 0.2946 - acc: 0.9062Epoch 39/50\n",
      "25938/25938 [==============================] - 24s 922us/step - loss: 0.2648 - acc: 0.9040 - val_loss: 3.4959 - val_acc: 0.4540\n",
      "25938/25938 [==============================] - 24s 922us/step - loss: 0.2648 - acc: 0.9040 - val_loss: 3.4959 - val_acc: 0.4540\n",
      "Epoch 40/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.1763 - acc: 0.9375Epoch 40/50\n",
      "25938/25938 [==============================] - 24s 924us/step - loss: 0.2695 - acc: 0.9032 - val_loss: 3.5401 - val_acc: 0.4631\n",
      "25938/25938 [==============================] - 24s 924us/step - loss: 0.2695 - acc: 0.9032 - val_loss: 3.5401 - val_acc: 0.4631\n",
      "Epoch 41/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.4425 - acc: 0.8438Epoch 41/50\n",
      "25938/25938 [==============================] - 24s 919us/step - loss: 0.2448 - acc: 0.9126 - val_loss: 3.5956 - val_acc: 0.4495\n",
      "25938/25938 [==============================] - 24s 919us/step - loss: 0.2448 - acc: 0.9126 - val_loss: 3.5956 - val_acc: 0.4495\n",
      "Epoch 42/50\n",
      "   32/25938 [..............................] - ETA: 29s - loss: 0.2456 - acc: 0.8750Epoch 42/50\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.2393 - acc: 0.9122 - val_loss: 3.6549 - val_acc: 0.4641\n",
      "25938/25938 [==============================] - 24s 921us/step - loss: 0.2393 - acc: 0.9122 - val_loss: 3.6549 - val_acc: 0.4641\n",
      "Epoch 43/50\n",
      "   32/25938 [..............................] - ETA: 29s - loss: 0.1090 - acc: 0.9375Epoch 43/50\n",
      "25938/25938 [==============================] - 24s 919us/step - loss: 0.2442 - acc: 0.9129 - val_loss: 3.6301 - val_acc: 0.4627\n",
      "25938/25938 [==============================] - 24s 919us/step - loss: 0.2442 - acc: 0.9129 - val_loss: 3.6301 - val_acc: 0.4627\n",
      "Epoch 44/50\n",
      "   32/25938 [..............................] - ETA: 28s - loss: 0.1666 - acc: 0.9375Epoch 44/50\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.2122 - acc: 0.9242 - val_loss: 3.7887 - val_acc: 0.4596\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.2122 - acc: 0.9242 - val_loss: 3.7887 - val_acc: 0.4596\n",
      "Epoch 45/50\n",
      "   32/25938 [..............................] - ETA: 26s - loss: 0.2211 - acc: 0.9062Epoch 45/50\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.2301 - acc: 0.9176 - val_loss: 3.7341 - val_acc: 0.4696\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.2301 - acc: 0.9176 - val_loss: 3.7341 - val_acc: 0.4696\n",
      "Epoch 46/50\n",
      "   32/25938 [..............................] - ETA: 23s - loss: 0.0498 - acc: 1.0000Epoch 46/50\n",
      "25938/25938 [==============================] - 24s 928us/step - loss: 0.2119 - acc: 0.9254 - val_loss: 3.8460 - val_acc: 0.4599\n",
      "25938/25938 [==============================] - 24s 928us/step - loss: 0.2119 - acc: 0.9254 - val_loss: 3.8460 - val_acc: 0.4599\n",
      "Epoch 47/50\n",
      "   32/25938 [..............................] - ETA: 28s - loss: 0.2580 - acc: 0.9375Epoch 47/50\n",
      "25938/25938 [==============================] - 24s 926us/step - loss: 0.1966 - acc: 0.9310 - val_loss: 3.8820 - val_acc: 0.4707\n",
      "25938/25938 [==============================] - 24s 926us/step - loss: 0.1966 - acc: 0.9310 - val_loss: 3.8820 - val_acc: 0.4707\n",
      "Epoch 48/50\n",
      "   32/25938 [..............................] - ETA: 25s - loss: 0.0526 - acc: 1.0000Epoch 48/50\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.2232 - acc: 0.9204 - val_loss: 3.8552 - val_acc: 0.4731\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.2232 - acc: 0.9204 - val_loss: 3.8552 - val_acc: 0.4731\n",
      "Epoch 49/50\n",
      "   32/25938 [..............................] - ETA: 27s - loss: 0.2583 - acc: 0.9375Epoch 49/50\n",
      "25938/25938 [==============================] - 24s 923us/step - loss: 0.2070 - acc: 0.9248 - val_loss: 3.9291 - val_acc: 0.4710\n",
      "25938/25938 [==============================] - 24s 923us/step - loss: 0.2070 - acc: 0.9248 - val_loss: 3.9291 - val_acc: 0.4710\n",
      "Epoch 50/50\n",
      "   32/25938 [..............................] - ETA: 29s - loss: 0.2557 - acc: 0.9375Epoch 50/50\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.1947 - acc: 0.9317 - val_loss: 4.0846 - val_acc: 0.4454\n",
      "25938/25938 [==============================] - 24s 925us/step - loss: 0.1947 - acc: 0.9317 - val_loss: 4.0846 - val_acc: 0.4454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce9589f7b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce9589f7b8>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= x/255.0\n",
    "model= Sequential()\n",
    "model.add(Conv2D(128,(3,3),input_shape= x.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer= \"adam\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x,y, batch_size=32,epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7yCvWfVhoC7"
   },
   "outputs": [],
   "source": [
    "model.save(\"faceExpression.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gw2FdFk1afHX"
   },
   "source": [
    "**We get Accuracy of 94%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IkAC_u6az67"
   },
   "source": [
    "**We can increase the accuracy by putting the image generation and dropout.**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Modal Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
